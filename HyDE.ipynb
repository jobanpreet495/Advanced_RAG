{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db6fcaf7-16e5-4e6b-a063-c60a119f8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9980b935-5766-4d85-8eac-d3c17c3af699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings  import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts  import PromptTemplate\n",
    "from langchain_openai  import  ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "470dacf1-44e7-4bb3-88c0-02a706f0d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(path, chunk_size=1000, chunk_overlap=200):\n",
    "    # Load PDF documents\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff97c7f7-de47-4b3f-8944-2fa1ef23bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/erginous/Desktop/attention.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "268ba641-f9f1-4fde-a559-4feabdf800f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9195/1374947802.py:12: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "vectorstore = process_document(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9008c51-a25f-4ff7-9e79-e2bd6af87a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-4o-mini\" , temperature = 0)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "document_len = 1000\n",
    "hyde_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"document_len\"],\n",
    "            template=\"\"\"Given the question '{query}', generate a hypothetical document that directly answers this question. The document should be detailed and in-depth.\n",
    "            the document size has be exactly {document_len} characters.\"\"\",\n",
    "        )\n",
    "\n",
    "hyde_chain = hyde_prompt | llm\n",
    "\n",
    "query = \"What is positional encoding ?\"\n",
    "hypothetical_docs = hyde_chain.invoke({\"query\":query , \"document_len\": document_len})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03217a20-a66a-4533-8271-c4953d81e924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**Positional Encoding: An In-Depth Overview**\\n\\nPositional encoding is a technique used in neural networks, particularly in transformer architectures, to provide information about the position of tokens in a sequence. Unlike recurrent neural networks (RNNs), which inherently process data in order, transformers process all tokens simultaneously. This parallel processing necessitates a method to retain the sequential order of the input data.\\n\\nPositional encodings are added to the input embeddings of tokens to inject this positional information. Typically, these encodings are generated using sine and cosine functions of varying frequencies. The formula for the positional encoding for a position \\\\( pos \\\\) and dimension \\\\( i \\\\) is:\\n\\n\\\\[\\nPE(pos, 2i) = \\\\sin\\\\left(\\\\frac{pos}{10000^{\\\\frac{2i}{d_{model}}}}\\\\right)\\n\\\\]\\n\\\\[\\nPE(pos, 2i+1) = \\\\cos\\\\left(\\\\frac{pos}{10000^{\\\\frac{2i}{d_{model}}}}\\\\right)\\n\\\\]\\n\\nHere, \\\\( d_{model} \\\\) is the dimensionality of the embeddings. This method allows the model to learn relationships between tokens based on their positions, facilitating better understanding of context and sequence in tasks like translation and text generation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 256, 'prompt_tokens': 47, 'total_tokens': 303, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'stop', 'logprobs': None}, id='run-25f7a756-7b6c-4acc-aa19-6794193dd060-0', usage_metadata={'input_tokens': 47, 'output_tokens': 256, 'total_tokens': 303})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothetical_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8b5857c-5b3e-402b-b927-7c1eb270c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_docs = vectorstore.similarity_search(hypothetical_docs.content , k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52a7ba25-2fe1-4e0f-841b-d751f031961c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "698bd4b5-5b49-4de8-899a-80efbb0f9b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/home/erginous/Desktop/attention.pdf', 'page': 5}, page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two'),\n",
       " Document(metadata={'source': '/home/erginous/Desktop/attention.pdf', 'page': 5}, page_content='PEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'source': '/home/erginous/Desktop/attention.pdf', 'page': 4}, page_content='encoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b1ae3-0b7c-414e-b999-d9cc105da1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
