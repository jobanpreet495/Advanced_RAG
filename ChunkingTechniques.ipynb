{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e83697-149b-4bd7-9b61-9750293689c0",
   "metadata": {},
   "source": [
    "## Introduction to Chunking\n",
    "Chunking is the process of breaking down large pieces of text into smaller, manageable segments for Large Language Model (LLM) applications. It's a critical technique in Retrieval Augmented Generation (RAG) systems that helps optimize the relevance of content retrieved from vector databases.\n",
    "\n",
    "\n",
    "## Why is chunking important?\n",
    "\n",
    "* LLMs have finite context windows\n",
    "* Helps maintain semantic coherence\n",
    "* Optimizes embedding quality\n",
    "* Improves retrieval accuracy\n",
    "* Enables better search results\n",
    "\n",
    "## Core Chunking Parameters\n",
    "Before diving into strategies, let's understand the key parameters:\n",
    "\n",
    "1. chunk_size: Maximum number of characters or tokens in each chunk\n",
    "2. chunk_overlap: Number of characters/tokens that overlap between consecutive chunks\n",
    "3. separator: Character(s) used to split the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc9878d-9549-4cb4-9b86-c0b7cb9ce308",
   "metadata": {},
   "source": [
    "## 1. Fixed-Size Chunking (Character-Based)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beedd4ba-67a2-4529-ae03-e3de095a02c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 118, which is longer than the specified 50\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "This is a long document that needs to be split into smaller chunks.\n",
    "Each chunk will have a maximum of 100 characters.\n",
    "\n",
    "This approach is simple but may cut sentences in the middle.\n",
    "We can use overlap to maintain some context between chunks.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",           # Split on double newlines\n",
    "    chunk_size=50,             # Max 50 characters per chunk\n",
    "    chunk_overlap=10,           # 10 characters overlap\n",
    "    length_function=len,        # Use character count\n",
    "    is_separator_regex=False    # Separator is not a regex\n",
    ")\n",
    "\n",
    "# Create documents\n",
    "docs = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eac53067-adfd-46eb-a274-17ee58cf97dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is a long document that needs to be split into smaller chunks.\\nEach chunk will have a maximum of 100 characters.'),\n",
       " Document(metadata={}, page_content='This approach is simple but may cut sentences in the middle.\\nWe can use overlap to maintain some context between chunks.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f181fbf-0286-44cd-a8bd-d89f5e6cf220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: This is a long document that needs to be split into smaller chunks.\n",
      "Each chunk will have a maximum of 100 characters.\n",
      "Length: 117\n",
      "---\n",
      "Chunk 2: This approach is simple but may cut sentences in the middle.\n",
      "We can use overlap to maintain some context between chunks.\n",
      "Length: 120\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Chunk {i+1}: {doc.page_content}\")\n",
    "    print(f\"Length: {len(doc.page_content)}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c591f11-83f8-4b90-a284-f7a2ee1530f3",
   "metadata": {},
   "source": [
    "## 2. Recursive Character Text Splitting\n",
    "\n",
    "##### How it works:\n",
    "\n",
    "* Tries to split by the first separator (\\n\\n)\n",
    "* If chunks are still too large, moves to the next separator (\\n)\n",
    "* Continues with space ( ) and finally empty string (\"\")\n",
    "* Recursively processes until desired chunk size is achieved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d25fa2be-5b6e-46ed-afbe-3e414b95f131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "'# Introduction\\nThis is a comprehensive guide to text splitting.'\n",
      "Length: 63\n",
      "---\n",
      "Chunk 2:\n",
      "'## Chapter 1: Basics\\nText splitting is fundamental to RAG applications.\\nIt helps in processing large documents efficiently.'\n",
      "Length: 123\n",
      "---\n",
      "Chunk 3:\n",
      "'## Chapter 2: Advanced Techniques\\nThere are several sophisticated methods available.\\nEach has its own advantages and use cases.'\n",
      "Length: 127\n",
      "---\n",
      "Chunk 4:\n",
      "'The choice depends on your specific requirements.'\n",
      "Length: 49\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"\"\"# Introduction\n",
    "This is a comprehensive guide to text splitting.\n",
    "\n",
    "## Chapter 1: Basics\n",
    "Text splitting is fundamental to RAG applications.\n",
    "It helps in processing large documents efficiently.\n",
    "\n",
    "## Chapter 2: Advanced Techniques\n",
    "There are several sophisticated methods available.\n",
    "Each has its own advantages and use cases.\n",
    "\n",
    "The choice depends on your specific requirements.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize recursive splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,             # Target chunk size\n",
    "    chunk_overlap=30,           # Overlap between chunks\n",
    "    length_function=len,        # Length measurement function\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Hierarchy of separators\n",
    ")\n",
    "\n",
    "# Split the text\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(repr(doc.page_content))\n",
    "    print(f\"Length: {len(doc.page_content)}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdb1936-5972-41cc-aeb0-68c64f6194f8",
   "metadata": {},
   "source": [
    "#### Language-Specific Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b45681c5-4c8f-4087-b5a0-192702f7a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import Language\n",
    "\n",
    "# Python code example\n",
    "python_code = '''\n",
    "class TextProcessor:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def process(self, text):\n",
    "        return text.upper()\n",
    "\n",
    "def main():\n",
    "    processor = TextProcessor(\"example\")\n",
    "    result = processor.process(\"hello world\")\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Split Python code with language-specific separators\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "docs = python_splitter.create_documents([python_code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acc4bb08-77e1-4588-b69f-58ca062844e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "'class TextProcessor:\\n    def __init__(self, name):\\n        self.name = name\\n    \\n    def process(self, text):\\n        return text.upper()'\n",
      "Length: 137\n",
      "---\n",
      "Chunk 2:\n",
      "'def main():\\n    processor = TextProcessor(\"example\")\\n    result = processor.process(\"hello world\")\\n    print(result)\\n\\nif __name__ == \"__main__\":\\n    main()'\n",
      "Length: 155\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(repr(doc.page_content))\n",
    "    print(f\"Length: {len(doc.page_content)}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4212633-68c4-44f3-bcd6-4222a1810eaa",
   "metadata": {},
   "source": [
    "## 3. Token-Based Splitting\n",
    "\n",
    "* Splits text based on tokens rather than characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2872ca-6e60-40d7-9489-49f2dc29d403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6e3d9a1-5424-4021-a180-36e13ee9febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text = \"\"\"\n",
    "\n",
    "Artificial Intelligence (AI) refers to the development of computer systems capable of performing tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, understanding language, and recognizing patterns. AI technologies are widely used in various industries such as healthcare, finance, transportation, and entertainment. For example, AI powers voice assistants like Siri and Alexa, recommends shows on Netflix, and helps doctors analyze medical images more accurately.\n",
    "\n",
    "AI systems are built using algorithms and large datasets, often relying on machine learning—a subset of AI that enables computers to improve their performance over time without being explicitly programmed. With advancements in deep learning and natural language processing, AI has made significant progress in areas like self-driving cars and chatbots.\n",
    "\n",
    "While AI offers many benefits, including efficiency and automation, it also raises ethical concerns around privacy, bias, and job displacement. Responsible development and regulation of AI are crucial to ensure it serves humanity positively.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Token-based splitting\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=100,      # 100 tokens per chunk\n",
    "    chunk_overlap=10     # 10 tokens overlap\n",
    ")\n",
    "\n",
    "docs = token_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6afffc7c-68f8-47d5-ae7a-d2c0f5951f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "'\\n\\nArtificial Intelligence (AI) refers to the development of computer systems capable of performing tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, understanding language, and recognizing patterns. AI technologies are widely used in various industries such as healthcare, finance, transportation, and entertainment. For example, AI powers voice assistants like Siri and Alexa, recommends shows on Netflix, and helps doctors analyze medical images more accurately.\\n\\nAI systems are built using algorithms and large datasets'\n",
      "Length: 579\n",
      "---\n",
      "Chunk 2:\n",
      "'\\nAI systems are built using algorithms and large datasets, often relying on machine learning—a subset of AI that enables computers to improve their performance over time without being explicitly programmed. With advancements in deep learning and natural language processing, AI has made significant progress in areas like self-driving cars and chatbots.\\n\\nWhile AI offers many benefits, including efficiency and automation, it also raises ethical concerns around privacy, bias, and job displacement. Responsible development and regulation of AI are crucial to ensure it'\n",
      "Length: 568\n",
      "---\n",
      "Chunk 3:\n",
      "' development and regulation of AI are crucial to ensure it serves humanity positively.\\n\\n\\n'\n",
      "Length: 89\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(repr(doc.page_content))\n",
    "    print(f\"Length: {len(doc.page_content)}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b466c9ea-94b4-4d5e-915e-ba6898649153",
   "metadata": {},
   "source": [
    "#### Using tiktoken for OpenAI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b708d44f-45e2-43bb-8f1e-c7bce331fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Using tiktoken encoder for OpenAI models\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100,     # 1000 tokens\n",
    "    chunk_overlap=10,   # 200 tokens overlap\n",
    "    encoding_name=\"cl100k_base\"  # GPT-4 encoding\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1903f539-0af3-4f3a-8fa1-c836ae3a4355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64d7f5e6-a346-4ce9-bbec-d948adc47145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "\n",
      "Artificial Intelligence (AI) refers to the development of computer systems capable of performing tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, understanding language, and recognizing patterns. AI technologies are widely used in various industries such as healthcare, finance, transportation, and entertainment. For example, AI powers voice assistants like Siri and Alexa, recommends shows on Netflix, and helps doctors analyze medical images more accurately.\n",
      "\n",
      "\n",
      "Chunk 2:\n",
      "\n",
      "AI systems are built using algorithms and large datasets, often relying on machine learning—a subset of AI that enables computers to improve their performance over time without being explicitly programmed. With advancements in deep learning and natural language processing, AI has made significant progress in areas like self-driving cars and chatbots.\n",
      "\n",
      "While AI offers many benefits, including efficiency and automation, it also raises ethical concerns around privacy, bias, and job displacement. Responsible development and regulation of AI are crucial to ensure it serves humanity positively.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(docs)):\n",
    "    print(f\"Chunk {i+1}:\\n\")\n",
    "    print(docs[i]+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6f30f-508f-42a9-80a8-d2cff218bceb",
   "metadata": {},
   "source": [
    "## 4. Semantic Chunking\n",
    "\n",
    "* This advanced technique splits text based on semantic similarity rather than fixed sizes, creating more contextually coherent chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7cc0264-1984-464e-a69c-defd63f883bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Chunk 1:\n",
      "\n",
      "Artificial intelligence is transforming industries worldwide. Machine learning algorithms can process vast amounts of data. Deep learning has revolutionized computer vision and NLP. Climate change poses significant challenges for our planet.\n",
      "---\n",
      "Semantic Chunk 2:\n",
      "Rising temperatures affect weather patterns globally. Renewable energy sources offer sustainable solutions. Space exploration continues to capture human imagination. Mars missions are planned for the next decade. Satellite technology improves communication systems. \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "text = \"\"\"\n",
    "Artificial intelligence is transforming industries worldwide.\n",
    "Machine learning algorithms can process vast amounts of data.\n",
    "Deep learning has revolutionized computer vision and NLP.\n",
    "\n",
    "Climate change poses significant challenges for our planet.\n",
    "Rising temperatures affect weather patterns globally.\n",
    "Renewable energy sources offer sustainable solutions.\n",
    "\n",
    "Space exploration continues to capture human imagination.\n",
    "Mars missions are planned for the next decade.\n",
    "Satellite technology improves communication systems.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize semantic chunker\n",
    "import  os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "semantic_splitter = SemanticChunker(embeddings)\n",
    "\n",
    "# Create semantically coherent chunks\n",
    "docs = semantic_splitter.create_documents([text])\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Semantic Chunk {i+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f2977-33d7-42e4-b095-47fc9b16af96",
   "metadata": {},
   "source": [
    "#### Different Semantic Splitting Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35eea1d1-feda-43f8-be51-fddc91d29472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Percentile method (default)\n",
    "percentile_splitter = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=95.0  # 95th percentile\n",
    ")\n",
    "\n",
    "# 2. Standard deviation method\n",
    "std_splitter = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"standard_deviation\",\n",
    "    breakpoint_threshold_amount=1.0  # 1 standard deviations\n",
    ")\n",
    "\n",
    "# 3. Interquartile range method\n",
    "iqr_splitter = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"interquartile\",\n",
    "    breakpoint_threshold_amount=1.5  # IQR multiplier\n",
    ")\n",
    "\n",
    "# 4. Gradient method (for highly correlated text)\n",
    "gradient_splitter = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"gradient\",\n",
    "    breakpoint_threshold_amount=95.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c75c2b95-2954-4542-8f9a-bdc77792bfc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='\\n\\nAI is transforming healthcare. Machine learning helps in disease prediction. NLP can extract data from medical records. Climate change is a serious issue.'),\n",
       " Document(metadata={}, page_content='Greenhouse gases are rising. Renewable energy is the solution. Mars missions are being planned. SpaceX has sent rockets to space. Satellites help track weather patterns. ')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text  =  \"\"\"\n",
    "\n",
    "AI is transforming healthcare.\n",
    "Machine learning helps in disease prediction.\n",
    "NLP can extract data from medical records.\n",
    "\n",
    "Climate change is a serious issue.\n",
    "Greenhouse gases are rising.\n",
    "Renewable energy is the solution.\n",
    "\n",
    "Mars missions are being planned.\n",
    "SpaceX has sent rockets to space.\n",
    "Satellites help track weather patterns.\n",
    "\"\"\"\n",
    "\n",
    "percentile_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6bc8d4b-ec53-4b70-bfbc-6e12a5e19436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='\\n\\nAI is transforming healthcare. Machine learning helps in disease prediction. NLP can extract data from medical records. Climate change is a serious issue.'),\n",
       " Document(metadata={}, page_content='Greenhouse gases are rising. Renewable energy is the solution. Mars missions are being planned.'),\n",
       " Document(metadata={}, page_content='SpaceX has sent rockets to space. Satellites help track weather patterns. ')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "349984a5-8ec7-4402-ba2e-504fd44e9883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='\\n\\nAI is transforming healthcare. Machine learning helps in disease prediction. NLP can extract data from medical records. Climate change is a serious issue.'),\n",
       " Document(metadata={}, page_content='Greenhouse gases are rising. Renewable energy is the solution. Mars missions are being planned.'),\n",
       " Document(metadata={}, page_content='SpaceX has sent rockets to space. Satellites help track weather patterns. ')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iqr_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494db050-b4e0-4bd4-8143-3534139c05c6",
   "metadata": {},
   "source": [
    "## 5. Document Structure-Aware Chunking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b443d-fd4b-4be6-bb89-361a36f0fd4d",
   "metadata": {},
   "source": [
    "#### Markdown Text Splitter\n",
    "Preserves markdown structure while splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53cd744e-3fbe-4bb1-9283-762c9238a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "markdown_text = \"\"\"\n",
    "# Main Title\n",
    "\n",
    "## Section 1\n",
    "This is the first section with important content.\n",
    "\n",
    "### Subsection 1.1\n",
    "More detailed information here.\n",
    "\n",
    "## Section 2\n",
    "This is the second section.\n",
    "\n",
    "### Subsection 2.1\n",
    "Additional details and examples.\n",
    "\"\"\"\n",
    "\n",
    "markdown_splitter = MarkdownTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "docs = markdown_splitter.create_documents([markdown_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18d542ec-3f18-40dd-861d-1d3a02a2d89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='# Main Title\\n\\n## Section 1\\nThis is the first section with important content.\\n\\n### Subsection 1.1\\nMore detailed information here.\\n\\n## Section 2\\nThis is the second section.'),\n",
       " Document(metadata={}, page_content='### Subsection 2.1\\nAdditional details and examples.')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b468f-ce21-4c84-a7a5-dbe8313cd127",
   "metadata": {},
   "source": [
    "#### HTML Header Text Splitter\n",
    "Splits HTML while preserving header hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ed0928d-5c00-4cf7-aa7d-ddbcb459cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Main Title</h1>\n",
    "        <p>Introduction paragraph</p>\n",
    "        \n",
    "        <h2>Chapter 1</h2>\n",
    "        <p>Content of chapter 1</p>\n",
    "        \n",
    "        <h3>Section 1.1</h3>\n",
    "        <p>Detailed content for section 1.1</p>\n",
    "        \n",
    "        <h2>Chapter 2</h2>\n",
    "        <p>Content of chapter 2</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Define headers to split on\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "\n",
    "html_header_splits = html_splitter.split_text(html_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c475403-ddd0-469e-9752-4f7570b81ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Main Title'}, page_content='Main Title'),\n",
       " Document(metadata={'Header 1': 'Main Title'}, page_content='Introduction paragraph'),\n",
       " Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Chapter 1'}, page_content='Chapter 1'),\n",
       " Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Chapter 1'}, page_content='Content of chapter 1'),\n",
       " Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Chapter 1', 'Header 3': 'Section 1.1'}, page_content='Section 1.1'),\n",
       " Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Chapter 1', 'Header 3': 'Section 1.1'}, page_content='Detailed content for section 1.1'),\n",
       " Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Chapter 2'}, page_content='Chapter 2'),\n",
       " Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Chapter 2'}, page_content='Content of chapter 2')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49b0c2-37c3-4417-946c-1bacc9677750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c3be4e5-70a6-4fae-874f-65f46c7f2064",
   "metadata": {},
   "source": [
    "## 6. Sentence-Based Chunking\n",
    "\n",
    "Splits text while respecting sentence boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b6d35-f5ba-4b7d-922c-ed38f263c1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a7e54eec-36e8-43da-a264-45adec9f78e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/erginous/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"\"\"\n",
    "Artificial intelligence is transforming industries worldwide.\n",
    "Machine learning algorithms can process vast amounts of data.\n",
    "Deep learning has revolutionized computer vision and NLP.\n",
    "\n",
    "Climate change poses significant challenges for our planet.\n",
    "Rising temperatures affect weather patterns globally.\n",
    "Renewable energy sources offer sustainable solutions.\n",
    "\n",
    "Space exploration continues to capture human imagination.\n",
    "Mars missions are planned for the next decade.\n",
    "Satellite technology improves communication systems.\n",
    "\"\"\"\n",
    "\n",
    "nltk_splitter = NLTKTextSplitter(\n",
    "    chunk_size=100 ,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "docs = nltk_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e61b7178-38b6-4e98-ab35-ec0abf2f54fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial intelligence is transforming industries worldwide.',\n",
       " 'Machine learning algorithms can process vast amounts of data.',\n",
       " 'Deep learning has revolutionized computer vision and NLP.',\n",
       " 'Climate change poses significant challenges for our planet.',\n",
       " 'Rising temperatures affect weather patterns globally.',\n",
       " 'Renewable energy sources offer sustainable solutions.',\n",
       " 'Space exploration continues to capture human imagination.',\n",
       " 'Mars missions are planned for the next decade.\\n\\nSatellite technology improves communication systems.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba53138-1778-4d3a-a8e5-450bda599a50",
   "metadata": {},
   "source": [
    "## 7. JSON and Structured Data Chunking\n",
    "For handling JSON and other structured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93135e62-bdc9-475e-b003-bdc72781300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"users\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"name\": \"Alice\",\n",
      "      \"email\": \"alice@example.com\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"name\": \"Bob\",\n",
      "      \"email\": \"bob@example.com\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "---\n",
      "{\n",
      "  \"products\": [\n",
      "    {\n",
      "      \"id\": 101,\n",
      "      \"name\": \"Laptop\",\n",
      "      \"price\": 999.99\n",
      "    },\n",
      "    {\n",
      "      \"id\": 102,\n",
      "      \"name\": \"Mouse\",\n",
      "      \"price\": 29.99\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveJsonSplitter\n",
    "import json\n",
    "\n",
    "# Sample JSON data\n",
    "json_data = {\n",
    "    \"users\": [\n",
    "        {\"id\": 1, \"name\": \"Alice\", \"email\": \"alice@example.com\"},\n",
    "        {\"id\": 2, \"name\": \"Bob\", \"email\": \"bob@example.com\"}\n",
    "    ],\n",
    "    \"products\": [\n",
    "        {\"id\": 101, \"name\": \"Laptop\", \"price\": 999.99},\n",
    "        {\"id\": 102, \"name\": \"Mouse\", \"price\": 29.99}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Initialize JSON splitter\n",
    "json_splitter = RecursiveJsonSplitter(max_chunk_size=50)\n",
    "\n",
    "# Split JSON data\n",
    "json_chunks = json_splitter.split_json(json_data=json_data)\n",
    "\n",
    "for chunk in json_chunks:\n",
    "    print(json.dumps(chunk, indent=2))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada825a-ff7c-4ac9-a4f1-a62123b68d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e3ab2-c745-4f81-8f2e-0934fb567f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3737848c-85ec-49a0-abe1-6d5e31e2d4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66699ca8-c229-4567-8bf9-0b4a9ece5468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39547d46-0409-4860-965d-ea19751987ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a724e-f678-4f18-94d3-dc0aa2217879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5ea28-26fd-4e72-a772-72a0db7306dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c3a326e-6c74-497c-b618-11ea61ad5515",
   "metadata": {},
   "source": [
    "Great! Let’s go deep into **Semantic Chunking** and the **4 threshold methods** with **real examples**, **mathematics**, and **visual intuition**. We’ll simulate everything step-by-step.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Step 1: What is Semantic Chunking?\n",
    "\n",
    "### 🧠 Goal:\n",
    "\n",
    "Split a document where **meaning shifts**.\n",
    "Rather than using fixed sizes, we use **semantic embeddings** to detect topic changes.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 How Semantic Chunking Works\n",
    "\n",
    "Let’s take a simple example:\n",
    "\n",
    "```text\n",
    "1. AI is transforming healthcare.\n",
    "2. Machine learning helps in disease prediction.\n",
    "3. NLP can extract data from medical records.\n",
    "\n",
    "4. Climate change is a serious issue.\n",
    "5. Greenhouse gases are rising.\n",
    "6. Renewable energy is the solution.\n",
    "\n",
    "7. Mars missions are being planned.\n",
    "8. SpaceX has sent rockets to space.\n",
    "9. Satellites help track weather patterns.\n",
    "```\n",
    "\n",
    "We’ll break this into **groups of 3 sentences**:\n",
    "\n",
    "| Group | Text                    |\n",
    "| ----- | ----------------------- |\n",
    "| G1    | Sentences 1–3 (AI)      |\n",
    "| G2    | Sentences 4–6 (Climate) |\n",
    "| G3    | Sentences 7–9 (Space)   |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 Step 2: Generate Embeddings\n",
    "\n",
    "Each group becomes a **vector** using embeddings.\n",
    "Let’s assume:\n",
    "\n",
    "* G1 = `[0.1, 0.2, 0.7]`\n",
    "* G2 = `[0.9, 0.8, 0.1]`\n",
    "* G3 = `[0.3, 0.2, 0.9]`\n",
    "\n",
    "These are **3D vectors** representing meaning.\n",
    "\n",
    "---\n",
    "\n",
    "## 📏 Step 3: Compute Pairwise Distances\n",
    "\n",
    "We compare the **cosine distance** between embeddings:\n",
    "\n",
    "### 🧮 Cosine Distance Formula:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_distance}(A, B) = 1 - \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
    "$$\n",
    "\n",
    "### Compute:\n",
    "\n",
    "#### Distance between G1 and G2:\n",
    "\n",
    "* A = G1 = `[0.1, 0.2, 0.7]`\n",
    "* B = G2 = `[0.9, 0.8, 0.1]`\n",
    "* Cosine similarity ≈ 0.42 → Distance = 1 - 0.42 = **0.58**\n",
    "\n",
    "#### G2 and G3:\n",
    "\n",
    "* Cosine similarity ≈ 0.76 → Distance = 0.24\n",
    "\n",
    "#### G1 and G3:\n",
    "\n",
    "* Cosine similarity ≈ 0.98 → Distance = 0.02\n",
    "\n",
    "So:\n",
    "\n",
    "```\n",
    "G1–G2: 0.58 (high distance → topic shift)\n",
    "G2–G3: 0.24 (medium)\n",
    "G1–G3: 0.02 (very similar)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 🔍 Step 4: Thresholding Methods in Action\n",
    "\n",
    "### Now let’s apply the 4 methods to this data:\n",
    "\n",
    "We assume distances between chunks:\n",
    "\n",
    "```python\n",
    "distances = [0.58, 0.24, 0.02]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 🎯 Percentile Method\n",
    "\n",
    "### Config:\n",
    "\n",
    "```python\n",
    "SemanticChunker(..., breakpoint_threshold_type=\"percentile\", breakpoint_threshold_amount=90.0)\n",
    "```\n",
    "\n",
    "### Math:\n",
    "\n",
    "Sort distances: `[0.02, 0.24, 0.58]`\n",
    "\n",
    "* 90th percentile = value below which 90% of distances fall = approx **0.52**\n",
    "\n",
    "### Rule:\n",
    "\n",
    "If distance > 0.52 → split\n",
    "\n",
    "✅ `0.58 > 0.52` → SPLIT\n",
    "❌ `0.24`, `0.02` → DON’T SPLIT\n",
    "\n",
    "**➡ Split between G1 and G2**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 📈 Standard Deviation Method\n",
    "\n",
    "### Config:\n",
    "\n",
    "```python\n",
    "breakpoint_threshold_type=\"standard_deviation\", breakpoint_threshold_amount=1.0\n",
    "```\n",
    "\n",
    "### Math:\n",
    "\n",
    "* Mean = $\\mu = \\frac{0.58 + 0.24 + 0.02}{3} ≈ 0.28$\n",
    "* Std dev = $\\sigma ≈ 0.23$\n",
    "\n",
    "### Threshold = μ + 1σ = `0.28 + 0.23 = 0.51`\n",
    "\n",
    "✅ `0.58 > 0.51` → SPLIT\n",
    "❌ Others → DON’T SPLIT\n",
    "\n",
    "**➡ Split between G1 and G2**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 📊 Interquartile Range (IQR) Method\n",
    "\n",
    "### Config:\n",
    "\n",
    "```python\n",
    "breakpoint_threshold_type=\"interquartile\", breakpoint_threshold_amount=1.5\n",
    "```\n",
    "\n",
    "### Math:\n",
    "\n",
    "Sorted distances: `[0.02, 0.24, 0.58]`\n",
    "\n",
    "* Q1 = 0.02, Q3 = 0.58\n",
    "* IQR = Q3 - Q1 = 0.56\n",
    "* Threshold = Q3 + 1.5 × IQR = `0.58 + 0.84 = 1.42`\n",
    "\n",
    "✅ No distance > 1.42 → No SPLIT\n",
    "\n",
    "**➡ No splits at all**\n",
    "\n",
    "*Note: You can reduce the multiplier (e.g., 1.0) to make it more sensitive.*\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 📐 Gradient Method\n",
    "\n",
    "### Idea:\n",
    "\n",
    "Looks at **change** between distances.\n",
    "\n",
    "* Let’s compute gradient (slope):\n",
    "\n",
    "```python\n",
    "grad_1 = 0.58 - 0.24 = 0.34\n",
    "grad_2 = 0.24 - 0.02 = 0.22\n",
    "```\n",
    "\n",
    "Let’s say we apply threshold = 0.3 (large change)\n",
    "\n",
    "✅ grad\\_1 = 0.34 → SPLIT\n",
    "❌ grad\\_2 = 0.22 → DON’T SPLIT\n",
    "\n",
    "**➡ Split between G1 and G2**\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Final Chunking Example:\n",
    "\n",
    "If only G1–G2 is split:\n",
    "\n",
    "* **Chunk 1**: G1 = Sentences 1–3 (AI)\n",
    "* **Chunk 2**: G2 + G3 = Sentences 4–9 (Climate + Space)\n",
    "\n",
    "Unless another split is detected, G2 and G3 remain grouped.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Summary of Behavior\n",
    "\n",
    "| Method        | Sensitive to                 | Splits Detected         |\n",
    "| ------------- | ---------------------------- | ----------------------- |\n",
    "| Percentile    | Top % dissimilarities        | G1–G2                   |\n",
    "| Std Deviation | Large outliers above average | G1–G2                   |\n",
    "| IQR           | Statistical outliers         | None (too conservative) |\n",
    "| Gradient      | Sudden jump in meaning       | G1–G2                   |\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Key Takeaways\n",
    "\n",
    "| Term                | Meaning                                                         |\n",
    "| ------------------- | --------------------------------------------------------------- |\n",
    "| **Cosine Distance** | Measures how different two chunks are in *meaning*              |\n",
    "| **Breakpoint**      | A place where semantic shift is large enough to justify a split |\n",
    "| **Threshold Type**  | The rule used to decide when a difference is \"too big\"          |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a **code example that prints embeddings, distances, and shows these splits live** using OpenAI embeddings and LangChain?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182de86a-939f-4b37-b1ea-a0ec2e413c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
