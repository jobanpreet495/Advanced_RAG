{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3c138dcb-bdd3-4bcb-b838-e328db76d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18396086-1bff-4ca3-b474-8e2f7b492116",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e0b1cbe0-e149-45fc-9e0e-b48cc55c65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings  import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts  import PromptTemplate\n",
    "from langchain_openai  import  ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7179fee0-f18d-46eb-927b-b5f50cb1cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(path, chunk_size=1000, chunk_overlap=200):\n",
    "    # Load PDF documents\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2645e34e-443c-4bb3-9c81-a598bc26329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/erginous/Desktop/attention.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "048c929d-e349-4eb1-b9f3-399895fe3ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = process_document(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9f895472-5dfc-40be-8f2c-f6404483f144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "class scoring(BaseModel):\n",
    "    score: float = Field(... ,description=\"The relevance score of a document corresponding to the query\")\n",
    "\n",
    "def rerank(query , docs , top_n=3):\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\", \"doc\"],\n",
    "        template=\"\"\"On a scale of 1-10, rate the relevance of the following document to the query. Consider the specific context and intent of the query, not just keyword matches.\n",
    "        Query: {query}\n",
    "        Document: {doc}\n",
    "        Relevance Score:\"\"\"\n",
    "    )\n",
    "    \n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "    llm_chain = prompt_template | llm.with_structured_output(scoring)\n",
    "    \n",
    "    scored_docs = []\n",
    "    for doc in docs:\n",
    "        input_data = {\"query\": query, \"doc\": doc.page_content}\n",
    "        score = llm_chain.invoke(input_data).score\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except ValueError:\n",
    "            score = 0  # Default score if parsing fails\n",
    "        scored_docs.append((doc, score))\n",
    "    \n",
    "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, _ in reranked_docs[:top_n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "11864392-3434-47fb-afde-97b70849cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is Positional Encoding ?\"\n",
    "initial_docs = vectorstore.similarity_search(query, k=10)\n",
    "reranked_docs = rerank(query, initial_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "25becf93-a9ab-433b-b07d-1f4a4a268d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top initial documents:\n",
      "\n",
      "Document 1:\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "learned and fixed [9].\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "PE(pos,2i)=sin(pos/100002i/d model)\n",
      "PE(pos,2i+1)=cos(pos/100002i/d model)\n",
      "where posis the position and iis the dimension. That is, each dimension of the positional encoding\n",
      "corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\n",
      "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      "relative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\n",
      "PEpos.\n",
      "We also experimented with using learned positional embeddings [ 9] instead, and found that the two\n",
      "\n",
      "Document 2:\n",
      "encoder.\n",
      "•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information flow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN( x) = max(0 , xW 1+b1)W2+b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "\n",
      "Document 3:\n",
      "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. nis the sequence length, dis the representation dimension, kis the kernel\n",
      "size of convolutions and rthe size of the neighborhood in restricted self-attention.\n",
      "Layer Type Complexity per Layer Sequential Maximum Path Length\n",
      "Operations\n",
      "Self-Attention O(n2·d) O(1) O(1)\n",
      "Recurrent O(n·d2) O(n) O(n)\n",
      "Convolutional O(k·n·d2) O(1) O(logk(n))\n",
      "Self-Attention (restricted) O(r·n·d) O(1) O(n/r)\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n"
     ]
    }
   ],
   "source": [
    "# print first 3 initial documents\n",
    "print(\"Top initial documents:\")\n",
    "for i, doc in enumerate(initial_docs[:3]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "300746cb-e66e-47cc-a494-dc17645f5cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is Positional Encoding ?\n",
      "\n",
      "Top reranked documents:\n",
      "\n",
      "Document 1:\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "learned and fixed [9].\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "PE(pos,2i)=sin(pos/100002i/d model)\n",
      "PE(pos,2i+1)=cos(pos/100002i/d model)\n",
      "where posis the position and iis the dimension. That is, each dimension of the positional encoding\n",
      "corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\n",
      "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      "relative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\n",
      "PEpos.\n",
      "We also experimented with using learned positional embeddings [ 9] instead, and found that the two\n",
      "\n",
      "Document 2:\n",
      "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. nis the sequence length, dis the representation dimension, kis the kernel\n",
      "size of convolutions and rthe size of the neighborhood in restricted self-attention.\n",
      "Layer Type Complexity per Layer Sequential Maximum Path Length\n",
      "Operations\n",
      "Self-Attention O(n2·d) O(1) O(1)\n",
      "Recurrent O(n·d2) O(n) O(n)\n",
      "Convolutional O(k·n·d2) O(1) O(logk(n))\n",
      "Self-Attention (restricted) O(r·n·d) O(1) O(n/r)\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "\n",
      "Document 3:\n",
      "PEpos.\n",
      "We also experimented with using learned positional embeddings [ 9] instead, and found that the two\n",
      "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
      "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
      "during training.\n",
      "4 Why Self-Attention\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
      "tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
      "(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "consider three desiderata.\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can\n",
      "be parallelized, as measured by the minimum number of sequential operations required.\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top reranked documents:\")\n",
    "for i, doc in enumerate(reranked_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc80085-6ea4-451c-9845-2c242effb532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
